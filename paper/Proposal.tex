\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}

\title{CS 350 Project Proposal}
\author{Konstantin Macarenco and Melanie Marks }
\date{February 2015}

\begin{document}

\maketitle


%Your proposal should identify:
%
%the name(s) of the student(s) working on the project;
%the choice of project topic;
%the implementation language;
%a list of the specific features that you expect to include in your final report; and
%a time plan that identifies at least 3 specific goals for each of the remaining 3 weeks of the term.
%A collaboration plan that describes how your team intends to work together.  


\noindent
\section*{Topic Choice:}

Good Sorts\\
Lots of options for algorithms and optimizations make this topic a good fit for this project, and studying algorithms.
%More optimization options (pivot points, partitions, ...).  Good to learn what optimizations make the biggest differences.\\
%expectation: different optimization are more suitable for different inputs
\section*{Implementation Language:}

We chose C and C++ as the languages for implementing the sorting algorithms because of their speed and low overhead, even though C/C++ is harder to implement, it will produce more accurate results. \\
On the other hand C is not well suited as a test harness, so we chose Python and Bash for test automatization.\\
%and bash for running those tests on our sorting implementations.\\

%To verify correctness for gathered results, and effectively measure execution run-time, various methods will be used, such as instrumenting code, profiling, comparison of estimated against actual run-time.\\ 
%Profiling tools such as "gprof"\\
\section*{Expected Features:} 
\begin{itemize}
\item Side by side comparison of Quicksort, Heapsort and Mergesort.
\item Customized sorts (using structs) for stability verification.
%Stability can be checked by using a custom struct that will contain two fields, for example, int value, int id, where id will be assigned during array initialization, and will correspond to item location within the array.
\item Crossover point compared to less efficient algorithms, such as Selection sort and Insertion sort.
\item Visual representation of results.
\item Visual comparisons of each of the sorting algorithms.
\item Comparison of space requirements for each algorithm.
\item (Quicksort) The impact of different pivot choosing methods - first vs middle vs random.
\item (Quicksort) The impact of different partitioning algorithms - Lomuto vs Hoare.
\item (Mergesort) Bottom-up vs top-down.
\item Verification of results.
\begin{itemize}
\item Comparing to language or system sort comparison.
\item Compare across our implementations.
\item Compare against predefined inputs (unsorted and sorted version of the same input).
\end{itemize}
\pagebreak
\item Runtime evaluation.
\begin{itemize}
\item Instrumenting source code.
\item Profiling.
\item Comparison of estimated and actual run-time.
\end{itemize}
\item Extra features, if time allows: 
\begin{itemize}
%\item Java vs C++, i.e virtual machine vs low level.
\item Multithreaded vs single threaded.
%TODO all tests, for consistency, should be performed on the same machine, maybe request a dedicated time for one of the linuxlab machines?
\item GPU
\end{itemize}
\end{itemize}
\section*{Specific Goals:} 
\begin{enumerate}
\item Week 1  Implementation
\begin{itemize}
\item Implement the Quicksort, Mergesort, and Heapsort algorithms.
\item Research improvements to implement in algorithms, especially for Mergesort and Heapsort since the class textbook has given fewer ideas regarding those algorithms.
\item Write algorithms in our own words for the report.
\item Mathematically estimate running times for each of the algorithms and compare to actual times.
\item Make note of any interesting implementation details or implemented algorithms not in the class textbook.
\end{itemize}
\item Week 2  Testing 
\begin{itemize}
\item Create test programs
\begin{itemize}
\item For deeper understanding of the studied field various inputs will be used. To keep results consistent all random input will be generated using the same seed, and  for generating Gaussian distribution we will use standard library. 
\item Test Inputs (usually lists of integers or floating-point numbers): 
\begin{itemize} \label{350:testinputs}
\item sorted
\item reverse sorted
\item randomized (Uniform distribution)
\item randomized (Gaussian/Normal distribution)
\item 25\% sorted
\item 85\% sorted
\item list of the identical items (Zero distribution)
\item test stability using customized sorting definition for list of inputs
\item more tests if time allows
\end{itemize}
\end{itemize}
\item Implement various improvements, including: 
\begin{itemize}
\item (Mergesort) dividing data into three (or more) sections and sorting those separately before merging
\item (Heapsort) dividing into multiple heaps
\end{itemize}
\item Test those improvements and document any changes in the algorithm's behavior. 
\item Document how the algorithms' space requirements compare.
\item Make note of unexpected results or improvements that we find to be especially creative or interesting.
\end{itemize}
\item Week 3  Report 
\begin{itemize}
\item Collect materials for report:
\begin{itemize}
\item Gather together the notes and documentation we have created over the course of the project.
\item Design graphs and other graphics to help display our results.
\end{itemize}
\item Collaboratively write report (using shared documentation system) and take turns editing drafts.
\end{itemize}
\end{enumerate}
\section*{Collaboration Plan: }
Along with meeting in person after class on Tuesdays and on Saturday afternoons, we have also created a shared github repository for our code and are using ShareLatex for our proposal, weekly notes, and final project report.  We will share the responsibility of writing code, analyzing algorithms, and writing the final report.
\end{document}
